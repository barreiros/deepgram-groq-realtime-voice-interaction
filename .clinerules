# Barreiros ThreeAI Project

## Project Overview

This project is a full-stack application that integrates Three.js for 3D rendering with real-time WebSocket communication and AI services. It features audio-to-audio communication through Gemini's multimodal API, with an architecture designed to support multiple AI service providers.

## Technology Stack

### Frontend

- **React**: JavaScript library for building user interfaces
- **Tailwind CSS**: Utility-first CSS framework with plugins (scrollbar, etc.)
- **ViteJS**: Fast, modern frontend build tool and development server
- **Three.js**: JavaScript 3D library for creating and displaying animated 3D computer graphics
- **WebSocket Client**: Native WebSocket API for real-time communication
- **Web Audio API**: For audio recording and playback
- **Audio Worklets**: For real-time audio processing

### Backend

- **Node.js**: JavaScript runtime for server-side code
- **Express.js**: Web application framework for Node.js
- **WebSocket Server**: Using the 'ws' library for WebSocket functionality
- **Gemini API**: Google's multimodal AI service for audio processing
- **AI Service Abstraction**: Architecture supporting multiple AI providers

## Project Structure

```
/
├── package.json         # Project configuration and dependencies
├── vite.config.js      # ViteJS configuration
├── .clinerules         # Project documentation
├── src/
│   ├── frontend/       # Frontend code
│   │   ├── index.html  # Main HTML file
│   │   ├── style.css   # CSS styles
│   │   ├── main.jsx    # React entry point
│   │   ├── audio/      # Audio processing
│   │   │   ├── AudioRecorder.js
│   │   │   ├── EventEmitter.js
│   │   │   ├── utils.js
│   │   │   └── worklets/    # Audio processing worklets
│   │   ├── components/ # React components
│   │   │   ├── App.jsx      # Root component
│   │   │   └── Scene.jsx    # Three.js scene
│   │   └── services/   # Frontend services
│   │       ├── audio/       # Audio services
│   │       ├── gemini/      # Gemini integration
│   │       └── websocket/   # WebSocket base service
│   └── backend/        # Backend code
│       ├── server.js   # Express server
│       ├── assistants/ # AI assistants configuration
│       └── services/   # Backend services
│           └── gemini/ # Gemini service implementation
```

## Features

1. **React Components**: Modular component architecture
2. **3D Visualization**: Interactive Three.js scene
3. **Real-time Audio**: Browser-based audio recording and playback
4. **AI Integration**: Gemini multimodal API with support for multiple providers
5. **Multiple Assistants**: Configurable AI assistants for different use cases
6. **Responsive Design**: Adapts to different screen sizes

## Implementation Details

### Component Guidelines

1. **Size Limit**: Components must not exceed 200 lines of code
2. **Single Responsibility**: Each component has one focused purpose
3. **Props Interface**: Clear prop types and documentation
4. **State Management**: Local state when possible, lifted when necessary
5. **Error Boundaries**: Required for critical components

### Audio Communication Flow

1. Client browser records audio using Web Audio API
2. Audio is processed through Audio Worklets
3. Processed audio is sent to backend via WebSocket
4. Backend forwards audio to Gemini API
5. Gemini's response audio is sent back to client
6. Client plays received audio through Audio Playback Service

### AI Service Architecture

1. **Base Service Interface**: Common interface for all AI providers
2. **Provider-Specific Services**: Implementations for each AI service (Gemini, future OpenAI, etc.)
3. **Assistant Configuration**: JSON-based configuration for different AI personas
4. **Service Factory**: Dynamic provider selection based on configuration

## Very very important

- At the end of the task you never should execute "npm run dev" or "open http://localhost..." because the project is already launched.
- Avoid to be verbose. It's not neccessary to explain your changes: just do it.
- Do not return other text than the neccessary code to change in the application, I dont need explanations.
- When you receive the command "close the task" or similar, you remove the consoles related with the task and commit the changes.
- When you receive the command "debug the flow" or similar you should add consoles to the code to have an output of the main values involved in the issue that we need to fix.
- Only output code, no text explanations.
